---
title: "Build a better training set"
subtitle: "Tidymodels, virtually"
session: 02
author: Alison Hill
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "assets/css/my-theme.css", "assets/css/my-fonts.css"]
    seal: false 
    lib_dir: libs
    nature:
      highlightLanguage: "r"
      highlightStyle: "xcode"
      slideNumberFormat: "" 
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes: 
      in_header:
        - 'assets/header.html'
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(comment = "#",
                      message = FALSE,
                      warning = FALSE, 
                      collapse = TRUE,
                      fig.retina = 3,
                      fig.align = 'center',
                      fig.path = "figs/rmed02-workflows/",
                      R.options = list(tibble.max_extra_cols=5, 
                                       tibble.print_max=5, 
                                       tibble.width=60))
options("scipen" = 16)
library(tidymodels)
yt_counter <- 0
```

```{r packages, include=FALSE}
library(countdown)
library(tidyverse)
library(tidymodels)
library(workflows)
library(scico)
library(gganimate)
library(AmesHousing)
library(tune)
library(viridis)
ames <- make_ames()
theme_set(theme_minimal())

set.seed(100) # Important!
ames_split  <- initial_split(ames)
ames_train  <- training(ames_split)
ames_test   <- testing(ames_split)

rt_spec <- 
  decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

# for figures
not_col <- scico(1, palette = "acton", begin = .6)
uni_col <- scico(1, palette = "acton", begin = 0)
train_color <- viridis(1, option="magma", begin = .4)
test_color  <- viridis(1, option="magma", begin = .7)
data_color  <- viridis(1, option="magma", begin = .1)
assess_color <- viridis(1, option="magma", begin = 1)
splits_pal <- c(data_color, train_color, test_color)

data("ad_data")
alz <- ad_data

# data splitting
set.seed(100) # Important!
alz_split  <- initial_split(alz, strata = Class, prop = .9)
alz_train  <- training(alz_split)
alz_test   <- testing(alz_split)

# data resampling
set.seed(100)
alz_folds <- 
    vfold_cv(alz_train, v = 10, strata = Class)
```



class: title-slide, center, bottom

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$subtitle` &mdash; Session `r stringr::str_pad(rmarkdown::metadata$session, 2, pad = "0")`

### `r rmarkdown::metadata$author` 


---
class: inverse, middle, center

# Build a better training set

# With recipes

---
class: middle, center, frame

# Recipes

```{r echo=FALSE, out.width="100%"}
knitr::include_url("https://recipes.tidymodels.org")
```

---
background-image: url(images/workflows/workflows.013.jpeg)
background-size: contain
background-position: center

---
class: middle, center, frame

# To build a recipe

1\. Start the `recipe()`

2\. Define the .display[variables] involved

3\. Describe **prep**rocessing .display[step-by-step]

---
class: middle, center

# `recipe()`

Creates a recipe for a set of variables

```{r eval=FALSE}
recipe(Class ~ ., data = alz)
```

---
class: middle

# .center[`recipe()`]

.center[Creates a recipe for a set of variables]

```{r eval=FALSE}
rec <- 
  recipe(Class ~ ., data = alz)
```

---
class: middle

# .center[`step_*()`]

.center[Adds a single transformation to a recipe. 
Transformations are replayed in order when the recipe is run on data.]

```{r}
rec <- 
  recipe(Class ~ ., data = alz) %>%
  step_other(Genotype, threshold = .03)
```

---

```{r echo=FALSE, fig.width=10}
alz %>% 
  count(Genotype) %>% 
  ggplot(aes(x = fct_reorder(Genotype, n, .desc = TRUE), 
             y = n)) +
  geom_col(fill = "#CA225E", alpha = .7) +
  coord_flip() +
  theme(text = element_text(family = "Lato")) +
  labs(x = "") +
  geom_hline(yintercept = (nrow(alz_train)*.03), lty = 3)
```

---

.pull-left[
## Before recipe
```{r}
alz_train %>% 
  count(Genotype, sort = TRUE)
```

]


.pull-right[

## After recipe
```{r}
rec %>% 
  prep() %>% 
  bake(new_data = alz_train) %>% 
  count(Genotype, sort = TRUE)
```

]
---
class: middle, center

# .center[`step_*()`]

Complete list at:
<https://recipes.tidymodels.org/reference/index.html>

```{r echo=FALSE, out.width="100%"}
knitr::include_url("https://recipes.tidymodels.org/reference/index.html")
```


---
class: middle, center, frame

# K Nearest Neighbors (KNN)

To predict the outcome of a new data point:

Find the K most similar old data points

Take the average/mode/etc. outcome

---
class: middle, frame

# .center[To specify a KNN model with parsnip]


```{r}
knn_mod <-
  nearest_neighbor() %>%              
  set_engine("kknn") %>%             
  set_mode("classification")        
```

---
class: middle, center

# Fact

KNN requires all numeric predictors, and all need to be **centered** and **scaled**. 

What does that mean?




---
class: middle

.center[
# Quiz

How does recipes know which variables are **numeric** and what is **nominal**?
]

--

```{r eval=FALSE}
rec <- 
  recipe(Class ~ ., 
         data = alz) #<<
```




---
class: middle, center

# Quiz

Why do you need to "train" a recipe?

--

Imagine "scaling" a new data point. What do you subtract from it? 
What do you divide it by?

---
background-image: url(images/pca.002.jpeg)
background-size: contain

---
background-image: url(images/pca.003.jpeg)
background-size: contain

---
background-image: url(images/pca.004.jpeg)
background-size: contain

---

```{r include=FALSE}
rec <- 
  recipe(Class ~ ., 
         data = alz) %>% 
  step_normalize(all_numeric()) 
```

```{r echo=FALSE}
rec %>% 
  prep(alz_train) %>%
  bake(alz_test) 
```



---

.center[

# Guess
]

.left-column[
```{r echo=FALSE, comment = NA}
alz %>%
  distinct(Genotype)
```
]

.right-column[
```{r echo=FALSE, comment = NA}
alz %>% 
  select(Genotype) %>% 
  mutate(val = 1, id = dplyr::row_number()) %>% 
  pivot_wider(id = id, 
              names_from = Genotype, 
              values_from = val, 
              values_fill = list(val = 0)) %>% 
  select(-id)
```

]

---
class: middle, center

# Dummy Variables

```{r results='hide'}
glm(Class ~ Genotype, family = "binomial", data = alz)
```

```{r echo=FALSE}
glm(Class ~ Genotype, family = "binomial", data = alz) %>% 
  broom::tidy()
```

---
class: middle

.center[
# `step_dummy()`

Converts nominal data into numeric dummy variables,
needed as predictors for models like KNN.

]

```{r results='hide'}
rec %>% 
  step_dummy(all_nominal(), -all_outcomes()) #<<
```

.footnote[You *don't* need this for decision trees or ensembles of trees]

---
class: middle

# .center[Combining selectors]

Use commas to separate

```{r eval=FALSE}
rec %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% #<<
  step_zv(all_predictors())
```



---
class: middle

.center[
# Quiz

How does recipes know what is a **predictor** and what is an **outcome**?
]
--

```{r eval=FALSE}
rec <-
  recipe(Sale_Price ~ ., #<<
         data = ames)
```

--

.center[The .display[formula] &rarr; *indicates outcomes vs predictors*]


--

.center[The .display[data] &rarr;  *is only used to catalog the names and types of each variable*]




---
class: middle

# .center[selectors]

Helper functions for selecting sets of variables

```{r eval=FALSE}
rec %>% 
  step_novel(all_nominal()) %>%
  step_zv(all_predictors())
```

---
class: middle

```{r include=FALSE}
all <- tribble(
  ~ selector, ~ description,
  "`all_predictors()`", "Each x variable  (right side of ~)",
  "`all_outcomes()`", "Each y variable  (left side of ~)",
  "`all_numeric()`", "Each numeric variable",
  "`all_nominal()`", "Each categorical variable (e.g. factor, string)",
  "`dplyr::select()` helpers", "`starts_with('IL_')`, etc."
)
```

```{r echo=FALSE, out.width='80%'}
library(gt)
gt(all)  %>%
  fmt_markdown(columns = TRUE) %>%
  tab_options(
    table.width = pct(10),
    table.font.size = "200px"
  )
```

---
class: middle, center

# Quiz

Let's think about the modeling. 

What if there were no individuals with `Genotype` E2E2 in the training data?

--

Will the model have a coefficient for `Genotype` E2E2?

--

.display[No]

--

What will happen if the test data has a home with a shed roof?

--

.display[Error!]

---
class: middle

.center[
# `step_novel()`

Adds a catch-all level to a factor for any new values not encountered in model training, 
which lets R intelligently predict new levels in the test set.

]

```{r results='hide'}
rec %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% #<<
  step_dummy(all_nominal(), -all_outcomes()) 
```

.footnote[Use *before* `step_dummy()` so new level is dummified]

---
class: middle, center

# Guess

What would happen if you try to normalize a variable that doesn't vary?

--

Error! You'd be dividing by zero!

---
class: middle

.center[
# `step_zv()`

Intelligently handles zero variance variables 
(variables that contain only a single value)

]


```{r results='hide'}
rec %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) #<<
```

---
class: middle

.center[
# `step_normalize()`

Centers then scales numeric variable (mean = 0, sd = 1)

]

```{r results='hide'}
rec <- 
  recipe(Class ~ ., 
         data = alz) %>% 
  step_normalize(all_numeric()) #<<
```
---
class: your-turn

# Your Turn `r (yt_counter <- yt_counter + 1)`

Put it all together! Make a new recipe for using a K-nearest neighbors model to classify impaired versus control patients; here are the steps:

1. Lumps all `Genotype`s with threshold < 
1. Adds a novel level to all factors  
2. Convert all factors to dummy variables  
3. Catches any zero variance variables  
4. Normalizes all of the predictors (centers and scales)
5. Computes the first 5 principal components  

Save the result as `knn_rec`

```{r echo=FALSE}
countdown(minutes = 5)
```

---
```{r}
knn_recipe <- 
  recipe(formula = Class ~ ., data = alz) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors(), -all_nominal()) 
knn_recipe
```

---

```{r}
summary(knn_recipe)
```

---
class: middle, center, frame

# usemodels

<https://tidymodels.github.io/usemodels/>

```{r echo=FALSE, out.width="100%"}
knitr::include_url("https://tidymodels.github.io/usemodels/")
```

---

```{r}
library(usemodels)
use_ranger(Class ~ ., data = alz, tune = FALSE)
use_kknn(Class ~ ., data = alz, verbose = TRUE, tune = FALSE)
```

---
class: center, middle, frame

# Axiom

Feature engineering and modeling are two halves of a single predictive workflow.

---
background-image: url(images/workflows/workflows.001.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.002.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.003.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.004.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.005.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.006.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.007.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.008.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.009.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.010.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.011.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.012.jpeg)
background-size: contain

---
background-image: url(images/workflows/workflows.013.jpeg)
background-size: contain


---
class: center, middle, inverse

# Workflows

---
class: middle, center

# `workflow()`

Creates a workflow to add a model and more to

```{r results='hide'}
workflow()
```

---
class: middle, center

# `add_formula()`

Adds a formula to a workflow `*`

```{r results='hide'}
workflow() %>% add_formula(Class ~ tau)
```

.footnote[`*` If you do not plan to do your own preprocessing]

---
class: middle, center

# `add_model()`

Adds a parsnip model spec to a workflow

```{r results='hide'}
workflow() %>% add_model(knn_mod)
```

---
background-image: url(images/zestimate.png)
background-position: center
background-size: contain

---
class: middle, center

# Guess

If we use `add_model()` to add a model to a workflow, what would we use to add a recipe?

--

Let's see!

---
class: your-turn

# Your Turn `r (yt_counter <- yt_counter + 1)`

Make a workflow that combines `pca_rec` and with `rt_spec`.

```{r echo=FALSE}
countdown(minutes = 1)
```

---

```{r}
knn_wf <-
  workflow() %>% 
  add_recipe(knn_recipe) %>% 
  add_model(knn_mod)
knn_wf
```


---
class: middle

.center[
# `add_recipe()`

Adds a recipe to a workflow.

]

```{r}
knn_wf <- 
  workflow() %>%
  add_recipe(knn_recipe) %>% #<<
  add_model(knn_mod)
```

---
class: middle

.center[
# Guess

Do you need to add a formula if you have a recipe?
]
--
.center[
Nope!
]
```{r}
rec <- 
  recipe(Class ~ ., #<<
         data = alz)
```

---
class: middle

.center[
# `fit()`

Fit a workflow that bundles a recipe`*` and a model.

]

```{r eval=FALSE}
_wf %>% 
  fit(data = alz_train) %>% 
  predict(alz_test)...
```


.footnote[`*` or a formula, if you do not plan to do your own preprocessing]


---
class: middle

.center[
# Preprocess k-fold resamples?

]

```{r}
set.seed(100)
alz_folds <- 
    vfold_cv(alz_train, v = 10, strata = Class)
```


---
class: middle

.center[
# `fit_resamples()`

Fit a workflow that bundles a recipe`*` and a model with resampling.

]

```{r eval=FALSE}
_wf %>% 
  fit_resamples(resamples = alz_folds)
```


.footnote[`*` or a formula, if you do not plan to do your own preprocessing]


---
class: your-turn

# Your Turn `r (yt_counter <- yt_counter + 1)`

Run the first chunk. Then try our KNN workflow on `alz_folds`. What is the ROC AUC?



```{r echo=FALSE}
countdown(minutes=3)
```

---

```{r}
set.seed(100)
alz_folds <- 
    vfold_cv(alz_train, v = 10, strata = Class)

knn_wf %>% 
  fit_resamples(resamples = alz_folds) %>% 
  collect_metrics()
```

---
class: middle, center

# Feature Engineering

.pull-left[
Before

![](https://media.giphy.com/media/Wn74RUT0vjnoU98Hnt/giphy.gif)
]

--

.pull-right[
After

![](https://media.giphy.com/media/108GZES8iG0myc/giphy.gif)
]

---
class: middle, frame

# .center[To specify a model with parsnip]



```{r}
tree_mod <- 
  decision_tree() %>% 
  set_engine(engine = "rpart") %>% 
  set_mode("classification")
```



---
```{r echo=FALSE}
library(parttree)
## Build our tree using parsnip (but with rpart as the model engine)
alz_tree_01 <-
  decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(Class ~ tau + VEGF, data = alz_train)

## Plot the data and model partitions
alz_train %>%
  ggplot(aes(x=tau, y=VEGF)) +
  geom_jitter(aes(col=Class), alpha=0.7) +
  geom_parttree(data = alz_tree_01, aes(fill=Class), alpha = 0.1) +
  theme_minimal() 
```

---


```{r echo=FALSE}
library(rattle)
fancyRpartPlot(alz_tree_01$fit, 
               sub = NULL,
               palettes = "RdPu")
```


---
```{r echo=FALSE}
## Build our tree using parsnip (but with rpart as the model engine)
alz_tree_02 <-
  decision_tree(min_n = 1, cost_complexity = 0, tree_depth = 20) %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(Class ~ tau + VEGF, data = alz_train)

## Plot the data and model partitions
alz_train %>%
  ggplot(aes(x=tau, y=VEGF)) +
  geom_jitter(aes(col=Class), alpha=0.7) +
  geom_parttree(data = alz_tree_02, aes(fill=Class), alpha = 0.1) +
  theme_minimal() 
```

---


```{r echo=FALSE}
knitr::include_graphics("figs/rmed02-workflows/big-alz-tree-1.png")
```


```{r big-alz-tree, include=FALSE, eval=FALSE}
library(rattle)
fancyRpartPlot(alz_tree_02$fit, 
               sub = NULL,
               palettes = "RdPu")
```

---

```{r}
set.seed(100)
tree_mod %>% 
  fit_resamples(Class ~ ., 
                resamples = alz_folds) %>% 
  collect_metrics()
```
